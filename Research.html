<!DOCTYPE HTML><html><head>  <title>Research</title>  <meta name="description" content="website description" />  <meta name="keywords" content="Zhanghui Kuang" />  <meta http-equiv="content-type" content="text/html; charset=windows-1252" />  <link rel="stylesheet" type="text/css" href="style/style.css" /></head><body>  <div id="main">    <div id="header">      <div id="logo">        <div id="logo_text">          <!-- class="logo_colour", allows you to change the colour of the text -->           <h1><a href="index.html">Zhanghui Kuang's<span class="logo_colour">Homepage</span></a></h1>          <h2></h2>        </div>        <div id="contact_details">          <p></p>          <p></p>        </div>      </div>      <div id="menubar">        <ul id="menu">          <!-- put class="selected" in the li tag for the selected page - to highlight which page you're on -->            <li ><a href="index.html">Home</a></li>          <li class="selected"><a href="Research.html">Research</a></li>          <li><a href="Publications.html">Publications</a></li>          <li><a href="Software.html">Software</a></li>          <li><a href="Personal.html">Personal</a></li>        </ul>      </div>    </div>    <div id="site_content">      <div class="sidebar">        <!-- insert your sidebar items here -->        <h3></h3>        <h4>Live as though you intend to live forever, and work as though your strength were limitless</h4>        <p text-align:right>--S. Bernhardt</p>        <ul>          <li><a href="http://www.cvchina.info/">cvChina</a></li>          <li><a href="http://www.cvpapers.com/">cvPapers</a></li>          <li><a href="http://nips.cc/">NIPS</a></li>          <li><a href="http://icml.cc/">ICML</a></li>        </ul>        <h3>Search</h3>        <form method="post" action="#" id="search_form">          <p>            <input class="search" type="text" name="search_field" value="Enter keywords....." />            <input name="search" type="image" style="border: 0; margin: 0 0 -9px 5px;" src="style/search.png" alt="Search" title="Search" />          </p>        </form>      </div>      <div id="content">        <!-- insert the page content here -->         <h1>Projects Summary</h1>         <a name="top"></a>        <table style="width:100%; border-spacing:0;">          <tr><th>Index</th><th>Name</th></tr>          <tr><td>1</td><td>  <a href="#p1">Learning image-specific parameters for interactive segmentation</a></td></tr>          <tr><td>2</td><td>  <a href="#p2">Relatively-Paired Space Analysis</a></td></tr>        </table>            <a name="p1"></a>        <h1>Learning image-specific parameters for interactive segmentation</h1>        <p>we present a novel interactive image segmentation techniquethat automatically learns segmentation parameters specifically tailored foreach and every image. Unlike existing work, our method does not require anyoffline parameter tuning or training stage, and is capable of determining image specificparameters according to simple user interaction with the target image.We formulate the segmentation problem as an inference of a conditional randomfield (CRF) over a segmentation mask and the target image, and parametrize thisCRF by different weights (e.g., color, texture and smoothing). The weight parametersis learned via an energy margin maximization, which is dealt with through aconstraint approximation scheme and the cutting plane method. Experimental resultsshow that our method, by learning image-specific parameters automatically,outperforms other state-of-the-art interactive image segmentation techniques.</p>        <h2>Features</h2>          <ul>          <li>Parameters is image-specific</li>          <li>Understand users' intention</li>          <li>Learn parameters from user interactions without a training dataset</li>        </ul>         <h2>Experimental Results</h2>         <table>        <tr>        <td> <center>Input</center></td>        <td> <center>0th iteration </center></td>        <td> <center>final iteration</center></td>        </tr>        <tr>        <td style="background:#FFFFFF">          <img  src="research/p1/fig1/31.jpg" width="180"          alt="profile picture">        </td>        <td style="background:#FFFFFF">           <img  src="research/p1/fig1/32.jpg" width="180"          alt="profile picture">        </td>         <td style="background:#FFFFFF">           <img  src="research/p1/fig1/33.jpg" width="180"          alt="profile picture">        </td>        </tr>         <tr>        <td style="background:#FFFFFF">          <img  src="research/p1/fig1/41.jpg" width="180"          alt="profile picture">        </td>        <td style="background:#FFFFFF">           <img  src="research/p1/fig1/42.jpg" width="180"          alt="profile picture">        </td>         <td style="background:#FFFFFF">           <img  src="research/p1/fig1/43.jpg" width="180"          alt="profile picture">        </td>        </tr>        </table>        Note that red, green, blue bars denote the weights of color, texture and smoothness respectively.        <h2>Demonstration</h2>        <p><center><iframe width="466" height="350" src="http://www.youtube.com/embed/i7SzWcE8tCA" frameborder="0" allowfullscreen></iframe></center></p>        <p><center>Go to <a href="#top">projects summary</a></center></p>        <h1>Relatively-Paired Space Analysis</h1>        <a name="p2"></a>        <p>Discovering a latent common space between different modalities plays an importantrole in cross-modality pattern recognition. Existing techniques often require absolutely-pairedobservations as training data, and are incapable of capturing more general semanticrelationships between cross-modality observations. This greatly limits their applications.In this paper, we propose a general framework for learning a latent commonspace from relatively-paired observations (i.e., two observations from different modalitiesare more-likely-paired than another two). Relative-pairing information is encodedusing relative proximities of observations in the latent common space. By building adiscriminative model and maximizing a distance margin, a projection function that mapsobservations into the latent common space is learned for each modality. Cross-modalitypattern recognition can then be carried out in the latent common space. To evaluate itsperformance, the proposed framework has been applied to cross-pose face recognitionand feature fusion. Experimental results demonstrate that the proposed framework outperformsother state-of-the-art approaches.</p> <h2>Experimental Results</h2>        <p><center>Go to <a href="#top">projects summary</a></center></p>      </div>    </div>     <div id="footer">      Last updated on July 4, 2013    </div>  <div style="text-align: center; font-size: 0.75em;">    Copyright &copy;    <a href="http://www.html5webtemplates.co.uk">HTML5webtemplates.co.uk</a>.  </div>  </div></body></html>